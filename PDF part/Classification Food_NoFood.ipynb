{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# **Classification Food/NoFood**<br/>\n","**Master's Degree in Data Science (A.Y. 2023/2024)**<br/>\n","**University of Milano - Bicocca**<br/>\n","\n","Vittorio Haardt, Luca Porcelli"],"metadata":{"id":"YNMDuV62zrNo"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"k6W0keP1zqtY","executionInfo":{"status":"ok","timestamp":1706871453523,"user_tz":-60,"elapsed":22115,"user":{"displayName":"Vittorio Haardt","userId":"02994810218058108833"}},"outputId":"40921964-180c-49f7-dfb7-7944aebdc694","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["# Data Extraction Train"],"metadata":{"id":"aPtjHRPxz-gn"}},{"cell_type":"code","source":["!unzip \"/content/drive/MyDrive/VIPM/Dataset/archive.zip\" -d train"],"metadata":{"id":"x2wZ6vpCkM_l"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Installing packages and loading libraries"],"metadata":{"id":"Urb72h87z4FG"}},{"cell_type":"code","source":["pip install efficientnet"],"metadata":{"id":"uo6vq9Z0c-el"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","import keras\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.applications import DenseNet121\n","from tensorflow.keras.layers import GlobalAveragePooling2D, BatchNormalization, Dropout, Dense\n","from tensorflow.keras import regularizers\n","from tensorflow.keras.models import Model\n","from tensorflow.keras import layers, models\n","from efficientnet.keras import EfficientNetB0\n","from PIL import Image\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from keras.callbacks import LearningRateScheduler, EarlyStopping, ModelCheckpoint"],"metadata":{"id":"g_ys6wRd9m8T"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Data Augmentation"],"metadata":{"id":"plWWtXHB0aYh"}},{"cell_type":"code","source":["# Creating an image generator with various transformations for data augmentation\n","datagen = ImageDataGenerator(\n","    rotation_range=40,          # Maximum rotation angle in degrees\n","    zoom_range=0.2,             # Random zoom\n","    horizontal_flip=True,       # Randomly flip images horizontally\n","    vertical_flip=True,         # Randomly flip images vertically\n","    brightness_range=[0.5, 1.5] # Range of brightness variation\n",")"],"metadata":{"id":"Ss4YB8rHbi_-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Train"],"metadata":{"id":"youXHei20Olq"}},{"cell_type":"code","source":["# Create a dataset of images from the specified directory\n","train = datagen.flow_from_directory(\n","    \"/content/train/archive/training\",    # Path to the directory containing training images\n","    classes=None,                         # Use default class directories\n","    class_mode=\"categorical\",             # Specify the encoding mode for labels\n","    color_mode=\"rgb\",                     # RGB format for images\n","    batch_size=64,                        # Batch size\n","    target_size=(224, 224),               # Set the dimensions of the images to 224x224 pixels\n","    shuffle=True,                         # Shuffle images within the dataset\n","    seed=777                              # Seed for reproducibility\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DEMULUxpbrS7","executionInfo":{"status":"ok","timestamp":1706268759079,"user_tz":-60,"elapsed":2,"user":{"displayName":"Luca Porcelli","userId":"09580591037146988201"}},"outputId":"8ee147a7-e50b-4da4-f669-0b15e795c497"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 3000 images belonging to 2 classes.\n"]}]},{"cell_type":"markdown","source":["# Validation"],"metadata":{"id":"UvPcXbug0dJ4"}},{"cell_type":"code","source":["# Create a dataset of images from the specified directory for validation\n","val = datagen.flow_from_directory(\n","    \"/content/train/archive/validation\",  # Path to the directory containing validation images\n","    classes=None,                         # Use default class directories\n","    class_mode=\"categorical\",             # Specify the encoding mode for labels\n","    color_mode=\"rgb\",                     # RGB format for images\n","    batch_size=64,                        # Batch size\n","    target_size=(224, 224),               # Set the dimensions of the images to 224x224 pixels\n","    shuffle=True,                         # Shuffle images within the dataset\n","    seed=777                              # Seed for reproducibility\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5jUvp7ITcCy2","executionInfo":{"status":"ok","timestamp":1706268761915,"user_tz":-60,"elapsed":322,"user":{"displayName":"Luca Porcelli","userId":"09580591037146988201"}},"outputId":"95bbe8f2-2d90-412e-bad0-b7bd27c03443"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 1000 images belonging to 2 classes.\n"]}]},{"cell_type":"markdown","source":["# EfficientNetB0"],"metadata":{"id":"IZ5IAxJ30lwi"}},{"cell_type":"markdown","source":["## Structure"],"metadata":{"id":"zBLu8Jy_0qyY"}},{"cell_type":"code","source":["# Creating the base EfficientNetB0 model pretrained on ImageNet\n","base_eff_model = EfficientNetB0(weights=\"imagenet\", include_top=False, input_shape=(224, 224, 3))\n","trainable_layer = False  # Setting the layers of the base model as non-trainable\n","\n","# Explicitly creating the input layer\n","inputs = keras.Input(shape=(224, 224, 3))\n","x = inputs\n","\n","# Preprocessing the input using the EfficientNetB0 preprocessing function\n","x = keras.applications.efficientnet.preprocess_input(x)\n","\n","# Passing the preprocessed input through the base model\n","x = base_eff_model(x)\n","\n","# GlobalAveragePooling2D to reduce spatial dimensions\n","x = keras.layers.GlobalAveragePooling2D()(x)\n","x = keras.layers.Dropout(0.3)(x)  # Dropout for regularization\n","\n","# Adding a dense layer for final classification with L2 regularization\n","outputs = keras.layers.Dense(2, activation='softmax', kernel_regularizer=keras.regularizers.l2(0.01))(x)\n","\n","# Creating the specialized EfficientNet model\n","Efficientnet = keras.Model(inputs=inputs, outputs=outputs)"],"metadata":{"id":"rv5TXRQw9Xvb","executionInfo":{"status":"ok","timestamp":1706268768067,"user_tz":-60,"elapsed":4865,"user":{"displayName":"Luca Porcelli","userId":"09580591037146988201"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"eb11590b-8261-446d-806f-11d4f766d86b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://github.com/Callidior/keras-applications/releases/download/efficientnet/efficientnet-b0_weights_tf_dim_ordering_tf_kernels_autoaugment_notop.h5\n","16804768/16804768 [==============================] - 0s 0us/step\n"]}]},{"cell_type":"code","source":["# Definition of a function for learning rate scheduling\n","def decay_schedule(epoch, lr):\n","    # Check if the epoch is a multiple of 5 and if the epoch is not zero\n","    if (epoch % 5 == 0) and (epoch != 0):\n","        # Reduce the learning rate by 20%\n","        lr = lr * 0.8\n","    # Return the new learning rate value\n","    return lr\n","\n","# Creating a LearningRateScheduler object using the learning rate schedule function\n","lr_scheduler = LearningRateScheduler(decay_schedule)\n","\n","# Early stopping callback\n","early_stop = EarlyStopping(\n","    monitor='val_loss',               # Monitors validation loss\n","    patience=5,                       # Number of epochs with no improvement before stopping\n","    verbose=1,                        # Prints messages about early stopping\n","    restore_best_weights=True         # Restores model weights from the epoch with the best value of the monitored quantity\n",")"],"metadata":{"id":"V29vIIlHeKi0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Training"],"metadata":{"id":"OIBmiIiJ0xG6"}},{"cell_type":"code","source":["# Compiling the model for training\n","Efficientnet.compile(\n","    loss='categorical_crossentropy',                                     # Categorical crossentropy loss for multi-class classification\n","    optimizer=Adam(0.0001),                                              # Adam optimizer with a learning rate of 0.0001\n","    metrics=['accuracy', tf.keras.metrics.TopKCategoricalAccuracy(k=5)]  # Metrics for evaluation during training\n",")\n","\n","# Training the model\n","history = Efficientnet.fit(\n","    train,                                # Training data generator\n","    epochs=50,                            # Number of training epochs\n","    validation_data=val,                  # Validation data generator\n","    callbacks=[lr_scheduler, early_stop]  # Callbacks for learning rate scheduling and early stopping\n",")"],"metadata":{"id":"A1bvFq1t9gJ1","colab":{"base_uri":"https://localhost:8080/"},"outputId":"0ffc2b1b-3f52-46bb-df9c-1fc80c8d8020","executionInfo":{"status":"ok","timestamp":1706271705579,"user_tz":-60,"elapsed":2668625,"user":{"displayName":"Luca Porcelli","userId":"09580591037146988201"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/50\n","47/47 [==============================] - 120s 2s/step - loss: 0.3510 - accuracy: 0.8773 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.4685 - val_accuracy: 0.7920 - val_top_k_categorical_accuracy: 1.0000 - lr: 1.0000e-04\n","Epoch 2/50\n","47/47 [==============================] - 79s 2s/step - loss: 0.1445 - accuracy: 0.9723 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.2568 - val_accuracy: 0.9240 - val_top_k_categorical_accuracy: 1.0000 - lr: 1.0000e-04\n","Epoch 3/50\n","47/47 [==============================] - 78s 2s/step - loss: 0.1027 - accuracy: 0.9817 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.1485 - val_accuracy: 0.9580 - val_top_k_categorical_accuracy: 1.0000 - lr: 1.0000e-04\n","Epoch 4/50\n","47/47 [==============================] - 79s 2s/step - loss: 0.0805 - accuracy: 0.9877 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.1167 - val_accuracy: 0.9770 - val_top_k_categorical_accuracy: 1.0000 - lr: 1.0000e-04\n","Epoch 5/50\n","47/47 [==============================] - 74s 2s/step - loss: 0.0831 - accuracy: 0.9883 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.1027 - val_accuracy: 0.9780 - val_top_k_categorical_accuracy: 1.0000 - lr: 1.0000e-04\n","Epoch 6/50\n","47/47 [==============================] - 76s 2s/step - loss: 0.0627 - accuracy: 0.9930 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0864 - val_accuracy: 0.9820 - val_top_k_categorical_accuracy: 1.0000 - lr: 8.0000e-05\n","Epoch 7/50\n","47/47 [==============================] - 75s 2s/step - loss: 0.0634 - accuracy: 0.9950 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0957 - val_accuracy: 0.9820 - val_top_k_categorical_accuracy: 1.0000 - lr: 8.0000e-05\n","Epoch 8/50\n","47/47 [==============================] - 77s 2s/step - loss: 0.0543 - accuracy: 0.9957 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0845 - val_accuracy: 0.9830 - val_top_k_categorical_accuracy: 1.0000 - lr: 8.0000e-05\n","Epoch 9/50\n","47/47 [==============================] - 74s 2s/step - loss: 0.0522 - accuracy: 0.9960 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0776 - val_accuracy: 0.9870 - val_top_k_categorical_accuracy: 1.0000 - lr: 8.0000e-05\n","Epoch 10/50\n","47/47 [==============================] - 76s 2s/step - loss: 0.0507 - accuracy: 0.9963 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0787 - val_accuracy: 0.9850 - val_top_k_categorical_accuracy: 1.0000 - lr: 8.0000e-05\n","Epoch 11/50\n","47/47 [==============================] - 76s 2s/step - loss: 0.0513 - accuracy: 0.9960 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0839 - val_accuracy: 0.9860 - val_top_k_categorical_accuracy: 1.0000 - lr: 6.4000e-05\n","Epoch 12/50\n","47/47 [==============================] - 74s 2s/step - loss: 0.0460 - accuracy: 0.9970 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0756 - val_accuracy: 0.9870 - val_top_k_categorical_accuracy: 1.0000 - lr: 6.4000e-05\n","Epoch 13/50\n","47/47 [==============================] - 76s 2s/step - loss: 0.0449 - accuracy: 0.9970 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0854 - val_accuracy: 0.9840 - val_top_k_categorical_accuracy: 1.0000 - lr: 6.4000e-05\n","Epoch 14/50\n","47/47 [==============================] - 75s 2s/step - loss: 0.0415 - accuracy: 0.9993 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0761 - val_accuracy: 0.9890 - val_top_k_categorical_accuracy: 1.0000 - lr: 6.4000e-05\n","Epoch 15/50\n","47/47 [==============================] - 77s 2s/step - loss: 0.0406 - accuracy: 0.9980 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0704 - val_accuracy: 0.9870 - val_top_k_categorical_accuracy: 1.0000 - lr: 6.4000e-05\n","Epoch 16/50\n","47/47 [==============================] - 75s 2s/step - loss: 0.0422 - accuracy: 0.9973 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0716 - val_accuracy: 0.9870 - val_top_k_categorical_accuracy: 1.0000 - lr: 5.1200e-05\n","Epoch 17/50\n","47/47 [==============================] - 77s 2s/step - loss: 0.0400 - accuracy: 0.9980 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0764 - val_accuracy: 0.9860 - val_top_k_categorical_accuracy: 1.0000 - lr: 5.1200e-05\n","Epoch 18/50\n","47/47 [==============================] - 74s 2s/step - loss: 0.0370 - accuracy: 0.9990 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0700 - val_accuracy: 0.9870 - val_top_k_categorical_accuracy: 1.0000 - lr: 5.1200e-05\n","Epoch 19/50\n","47/47 [==============================] - 76s 2s/step - loss: 0.0400 - accuracy: 0.9980 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0627 - val_accuracy: 0.9870 - val_top_k_categorical_accuracy: 1.0000 - lr: 5.1200e-05\n","Epoch 20/50\n","47/47 [==============================] - 74s 2s/step - loss: 0.0359 - accuracy: 0.9993 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0712 - val_accuracy: 0.9880 - val_top_k_categorical_accuracy: 1.0000 - lr: 5.1200e-05\n","Epoch 21/50\n","47/47 [==============================] - 77s 2s/step - loss: 0.0366 - accuracy: 0.9987 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0711 - val_accuracy: 0.9850 - val_top_k_categorical_accuracy: 1.0000 - lr: 4.0960e-05\n","Epoch 22/50\n","47/47 [==============================] - 75s 2s/step - loss: 0.0378 - accuracy: 0.9987 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0732 - val_accuracy: 0.9850 - val_top_k_categorical_accuracy: 1.0000 - lr: 4.0960e-05\n","Epoch 23/50\n","47/47 [==============================] - 76s 2s/step - loss: 0.0326 - accuracy: 0.9990 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0611 - val_accuracy: 0.9870 - val_top_k_categorical_accuracy: 1.0000 - lr: 4.0960e-05\n","Epoch 24/50\n","47/47 [==============================] - 74s 2s/step - loss: 0.0357 - accuracy: 0.9987 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0624 - val_accuracy: 0.9880 - val_top_k_categorical_accuracy: 1.0000 - lr: 4.0960e-05\n","Epoch 25/50\n","47/47 [==============================] - 77s 2s/step - loss: 0.0361 - accuracy: 0.9983 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0683 - val_accuracy: 0.9880 - val_top_k_categorical_accuracy: 1.0000 - lr: 4.0960e-05\n","Epoch 26/50\n","47/47 [==============================] - 75s 2s/step - loss: 0.0341 - accuracy: 0.9983 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0608 - val_accuracy: 0.9860 - val_top_k_categorical_accuracy: 1.0000 - lr: 3.2768e-05\n","Epoch 27/50\n","47/47 [==============================] - 76s 2s/step - loss: 0.0344 - accuracy: 0.9983 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0766 - val_accuracy: 0.9850 - val_top_k_categorical_accuracy: 1.0000 - lr: 3.2768e-05\n","Epoch 28/50\n","47/47 [==============================] - 74s 2s/step - loss: 0.0333 - accuracy: 0.9980 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0597 - val_accuracy: 0.9850 - val_top_k_categorical_accuracy: 1.0000 - lr: 3.2768e-05\n","Epoch 29/50\n","47/47 [==============================] - 76s 2s/step - loss: 0.0314 - accuracy: 0.9990 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0544 - val_accuracy: 0.9900 - val_top_k_categorical_accuracy: 1.0000 - lr: 3.2768e-05\n","Epoch 30/50\n","47/47 [==============================] - 75s 2s/step - loss: 0.0349 - accuracy: 0.9980 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0567 - val_accuracy: 0.9870 - val_top_k_categorical_accuracy: 1.0000 - lr: 3.2768e-05\n","Epoch 31/50\n","47/47 [==============================] - 75s 2s/step - loss: 0.0305 - accuracy: 0.9993 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0618 - val_accuracy: 0.9870 - val_top_k_categorical_accuracy: 1.0000 - lr: 2.6214e-05\n","Epoch 32/50\n","47/47 [==============================] - 75s 2s/step - loss: 0.0301 - accuracy: 0.9987 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0660 - val_accuracy: 0.9850 - val_top_k_categorical_accuracy: 1.0000 - lr: 2.6214e-05\n","Epoch 33/50\n","47/47 [==============================] - 76s 2s/step - loss: 0.0326 - accuracy: 0.9967 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0613 - val_accuracy: 0.9860 - val_top_k_categorical_accuracy: 1.0000 - lr: 2.6214e-05\n","Epoch 34/50\n","47/47 [==============================] - ETA: 0s - loss: 0.0313 - accuracy: 0.9980 - top_k_categorical_accuracy: 1.0000Restoring model weights from the end of the best epoch: 29.\n","47/47 [==============================] - 74s 2s/step - loss: 0.0313 - accuracy: 0.9980 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0606 - val_accuracy: 0.9890 - val_top_k_categorical_accuracy: 1.0000 - lr: 2.6214e-05\n","Epoch 34: early stopping\n"]}]},{"cell_type":"markdown","source":["## Saving model"],"metadata":{"id":"H1liWm9004Le"}},{"cell_type":"code","source":["# Saving the trained model\n","Efficientnet.save('/content/drive/MyDrive/VIPM/Models/Model_PDF.h5')\n","Efficientnet.save('/content/drive/MyDrive/VIPM/Models/Model_PDF.Keras')"],"metadata":{"id":"adU4fLhHksRH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Evaluation"],"metadata":{"id":"eEhQpVUJZN89"}},{"cell_type":"code","source":["# Creating a test dataset using tf.keras.utils.image_dataset_from_directory\n","test = tf.keras.utils.image_dataset_from_directory(\n","    \"/content/train/archive/evaluation\",     # Path to the directory containing evaluation images\n","    labels=\"inferred\",                       # Infers labels from the directory structure\n","    label_mode=\"categorical\",                # Specify the encoding mode for labels\n","    color_mode=\"rgb\",                        # RGB format for images\n","    batch_size=64,                           # Batch size\n","    image_size=(224, 224),                   # Set the dimensions of the images to 224x224 pixels\n","    shuffle=True                             # Shuffle images within the dataset\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"08JLd6RKDTse","executionInfo":{"status":"ok","timestamp":1706271771245,"user_tz":-60,"elapsed":1884,"user":{"displayName":"Luca Porcelli","userId":"09580591037146988201"}},"outputId":"f4501cf5-f750-48af-8204-fe463efa2ceb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 1000 files belonging to 2 classes.\n"]}]},{"cell_type":"code","source":["Efficientnet.evaluate(test)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g8r2rBi0DabO","executionInfo":{"status":"ok","timestamp":1706271788926,"user_tz":-60,"elapsed":10649,"user":{"displayName":"Luca Porcelli","userId":"09580591037146988201"}},"outputId":"1e632aca-a9bc-40a5-b7a7-58143b58bc0e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["16/16 [==============================] - 6s 194ms/step - loss: 0.0586 - accuracy: 0.9890 - top_k_categorical_accuracy: 1.0000\n"]},{"output_type":"execute_result","data":{"text/plain":["[0.05862918123602867, 0.9890000224113464, 1.0]"]},"metadata":{},"execution_count":13}]},{"cell_type":"markdown","source":["**Reference**\n","- Dataset: [Food5k](https://www.kaggle.com/datasets/trolukovich/food5k-image-dataset)"],"metadata":{"id":"MdpHOzcUOW8M"}}]}